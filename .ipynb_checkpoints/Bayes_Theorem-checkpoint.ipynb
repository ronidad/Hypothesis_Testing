{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bgp1DV4UuWzW"
   },
   "source": [
    "<font color=\"green\">*To start working on this notebook, or any other notebook that we will use in the Moringa Data Science Course, we will need to save our own copy of it. We can do this by clicking File > Save a Copy in Drive. We will then be able to make edits to our own copy of this notebook.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8flRWHtSHki3"
   },
   "source": [
    "# Python Programming: Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96w9G0XWJrC3"
   },
   "source": [
    "The Bayes Theorem is applicable in machine learning where we get to use a Bayes classifier inorder to make a prediction. In this session, we will learn how to apply this classifer to a few machine learning problems even though later during Core we will spent time exhaustively on working on such problems. While working, we should note that the bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Such classifiers, Naive Bayes classifiers, are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhgc8dlkL_UT"
   },
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lBLmRc5HgqE"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "# ---\n",
    "# Let's see an overview on how this classifier works, which suitable applications it has, \n",
    "# and how to use it in just a few lines of Python and the Scikit-Learn library.\n",
    "# ---\n",
    "# Question: Build a very simple SPAM detector for SMS messages given the following dataset; \n",
    "# ---\n",
    "# Dataset source = https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxW80SGJcwPP"
   },
   "outputs": [],
   "source": [
    "# Importing our library\n",
    "# ---\n",
    "#\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF7rixbGclp_"
   },
   "outputs": [],
   "source": [
    "# Loading our uploaded Data\n",
    "# ---\n",
    "# We define a separator (in this case, a tab) and rename the columns accordingly\n",
    "# \n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'], encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgQU75Rhc2i2"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 1. Converting the labels from strings to binary values for our classifier\n",
    "# \n",
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXLkppitgQ7A"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 2. Converting all characters in the message to lower case:\n",
    "# \n",
    "df['message'] = df.message.map(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG3j0ymwgWOx"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 3. Remove any punctuation:\n",
    "# \n",
    "df['message'] = df.message.str.replace('[^\\w\\s]', '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0B-lfLPgivV"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 4. tokenize the messages into into single words using nltk. \n",
    "# First, we have to import and download the tokenizer from the console:\n",
    "# \n",
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ttknwa9guS4"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 5. Applying the tokenization. \n",
    "# What is tokenization (http://bit.ly/WhatisTokenization)\n",
    "# \n",
    "df['message'] = df['message'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kWxzerKg0V6"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 6. We then perform some word stemming. \n",
    "# The idea of stemming is to normalize our text for all variations of words carry the same meaning, \n",
    "# regardless of the tense. One of the most popular stemming algorithms is the Porter Stemmer:\n",
    "# \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXv3cwwrhAWw"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 7. We will transform the data into occurrences, \n",
    "# which will be the features that we will feed into our model:\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This converts the list of words into space-separated strings\n",
    "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VERkaCMThK8I"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# ---\n",
    "# 8. We could leave it as the simple word-count per message, \n",
    "# but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf:\n",
    "#\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "\n",
    "counts = transformer.transform(counts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vksf5PobhVQ-"
   },
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "# ---\n",
    "# Now that we have performed feature extraction from our data, \n",
    "# it is time to build our model. We will start by splitting our data into training and test sets:\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUWlrPzwhfAb"
   },
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "# ---\n",
    "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
    "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited:\n",
    "# \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT0zZNSkhpvM"
   },
   "outputs": [],
   "source": [
    "# Evaluating the Model\n",
    "# ---\n",
    "# Once we have put together our classifier, we can evaluate its performance in the testing set:\n",
    "#\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))\n",
    "\n",
    "# Our simple Naive Bayes Classifier has 94.8% accuracy with this specific test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Lgaf8JCiI-L"
   },
   "source": [
    "## <font color=\"green\">Challenges</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG5nw1kRtU0g"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "# ---\n",
    "# In this challenge, we have been tasked with creating a classifier, the training set,\n",
    "# then training the classifier using the training set and making a prediction.\n",
    "# ---\n",
    "# The training set (X) consits of length, weight and shoe size. \n",
    "# Y contains the associated labels (male or female).\n",
    "# \n",
    "\n",
    "X = [[121, 80, 44], [180, 70, 43], [166, 60, 38], [153, 54, 37], [166, 65, 40], [190, 90, 47], [175, 64, 39],\n",
    "     [174, 71, 40], [159, 52, 37], [171, 76, 42], [183, 85, 43]]\n",
    "\n",
    "Y = ['male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male']\n",
    "\n",
    "# Training the classifier:\n",
    "#\n",
    "OUR CODE GOES HERE\n",
    "\n",
    "# Making the prediciton:\n",
    "# Using the GaussianNB classifier (i.e. from sklearn.naive_bayes import GaussianNB) \n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHn_lBT8iPVY"
   },
   "outputs": [],
   "source": [
    "# Example 2\n",
    "# ---\n",
    "# Question: Use the titanic disaster dataset to create a Gaussian Naive Bayes classifier model \n",
    "# (i.e. from sklearn.naive_bayes import GaussianNB) that will make a prediction of survival \n",
    "# using passenger ticket fare information. \n",
    "# ---\n",
    "# Dataset url: http://bit.ly/TitanicDataset \n",
    "# \n",
    "OUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZizDQj7iQ4O"
   },
   "outputs": [],
   "source": [
    "# Example 3\n",
    "# ---\n",
    "# Question: Create a GaussianNB classifier (i.e. from sklearn.naive_bayes import GaussianNB) \n",
    "# to identify the different species of iris flowers.\n",
    "# ---\n",
    "# Dataset url = http://bit.ly/MSIrisDatasetNB\n",
    "# \n",
    "OUR CODE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Mhgc8dlkL_UT",
    "5Lgaf8JCiI-L"
   ],
   "name": "Python Programming: Bayes Theorem",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
